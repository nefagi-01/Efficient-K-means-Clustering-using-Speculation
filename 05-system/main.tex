In the simplest implementation of Speculative K-Means, there are two threads: the fast execution thread ($t_f$) and the slow execution thread ($t_s$). The $t_s$ thread acts as the main thread and is responsible for initializing the K-Means algorithm, sampling the initial centroids and comparing the centroids.
At each stage, $t_s$ communicates the centroids from which to start to $t_f$. At this point, they begin working in parallel, with $t_s$ executing one Assignment and one Update step and $t_f$ executing many. Once $t_s$ finishes, it sends a signal to $t_f$ to stop its execution. Both $t_f$ and $t_s$ then start computing the inertia of their own centroids in parallel. Finally, $t_f$ communicates its centroids and the respective inertia to $t_s$, and $t_s$ compares the results and chooses the best ones. $t_s$ then communicates the chosen centroids to $t_f$, and the process is repeated.

If we have access to numerous resources, there are two possible approaches we can take: we can either keep the existing architecture and add parallelism to both the fast and slow execution to speed up their execution times, or we can add more fast execution threads. The first approach involves combining parallelism and speculation in order to more quickly process each stage in both the slow and fast execution. The second approach aims to improve the predictive power of the fast execution by adding more threads. Each of these threads would start from the same centroids, and they would work on a different subset of the data, resulting in different speculated centroids. We can either combine these centroids into a single proposal or present them separately and choose the best one. The combination of the centroids obtained addresses the issue of the accuracy when working with a sample of the dataset, since it combines the results obtained from different subsamples. On the other hand, if we are also using centroid resampling, it would be better to choose the best proposal rather than averaging them, as each proposal represents an opportunity to reach the global minimum. Finally, we can also consider combining all these techniques if we have numerous resources, adding parallelization in the slow execution and in several fast executions.