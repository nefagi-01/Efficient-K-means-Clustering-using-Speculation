\label{section:related_work}

K-Means clustering is a widely used unsupervised machine learning algorithm for data partitioning and clustering. This section provides an overview of the basic K-Means algorithms, the existing initialization techniques, the approximations which can be done to improve performances and the parallelization methods. We discuss the advantages and disadvantages of various approaches and explain how they can be applied in various scenarios. Furthermore, we present a comparison between the currently used approaches and the one we propose based on Speculation. Finally, we also show how our proposed approach can outperform the existing ones in terms of both accuracy and speed.
\subsection{K-Means++ initialization}
\label{section:K-Means_pp}

K-Means is sensible to initialization problems. It starts by randomly assigning a set of centroids, which can lead to suboptimal solutions if they are not well-chosen. To mitigate this, it is important to use an appropriate initialization strategy, such as k-means++, which elects initial cluster centroids using sampling based on an empirical probability distribution that reflects the points' contribution to the overall inertia. This offers a number of advantages over random initialization. It helps to avoid local minima, meaning that it can find the global optimum and produce better quality clusters compared to random initialization. Additionally, it may help to reduce the number of iterations needed to find the optimal solution, making it faster and more efficient than random initialization.
In Algorithm \ref{alg:K-Means_pp_initialization} we describe the steps for the centroid initialization.
\begin{algorithm}[H]
  \caption{K-Means++ Initialization}
  \label{alg:K-Means_pp_initialization}
  \begin{algorithmic}[1]
    \State Choose one center uniformly at random among the data points.
    \For{each data point x not chosen yet}
        \label{step:compute_Dx}
        \State Compute $D(x)$, the distance between x and the nearest center that has already been chosen.
    \EndFor
    \State Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to $D(x)^2$.
    \label{step:choose_centroid}
    \Repeat
        \State Steps \ref{step:compute_Dx} to \ref{step:choose_centroid}
    \Until k centers have not been chosen
    \State Return the initial centers have been chosen
  \end{algorithmic}
\end{algorithm}

We can see how the initialization Algorithm \ref{alg:K-Means_pp_initialization} involves the computation of the distances between each datapoint and the nearest center. This is an expensive operation, which is repeated every time a new centroid is sampled. Therefore, all the aforementioned benefits come to an initial time-cost we need to pay before running the actual algorithm. This can be prohibitive in particular applications, especially in case of large datasets.

\subsection{Approximations with Sampling. Mini-batch K-Means and online K-Means.}
\label{section:approximation}
Approximations with sampling are methods for increasing performances by using a subset of the data. In addition, these algorithms are useful when dealing with large datasets that would be too costly to process in one go. 
Mini-batch K-Means and online K-Means are two such methods. 
\begin{itemize}
    \item Mini-batch K-Means is an iterative algorithm that uses a subset of the data to update the cluster centers. At each K-Means iteration a new mini-batch is sampled.
    \item Online K-Means is a streaming algorithm that updates the cluster centers using one datapoint at the time.
\end{itemize}
Both of these methods are used to reduce the execution time and the amount of memory needed to run K-means, especially in case of big datasets. However, as we saw in section \ref{section:sampling}, this comes at a cost on the accuracy of the final centroids that are found. The fact we are not using the entire dataset in order to find the centroids leads to solutions with a higher inertia than what we could have obtained by using the entire dataset.

\subsection{Parallelization}
\label{section:parallelization}
An alternative solution for addressing K-Means on large datasets is to use parallelization with a distributed framework such as Spark. This approach takes advantage of distributed computing to process large datasets quickly and efficiently. However, there are two limitations to this approach, as discussed in [paper speculation]. Firstly, Amdahl's law suggests that the benefits of additional resources may be limited, as the increase in resources often results in diminishing returns in performance. Secondly, the data parallelization cannot break the dependency between the two steps in K-means mentioned in section \ref{section:dependencies}, meaning that a single step must be executed at a time, possibly leading to resource underutilization. In fact, in some scenarios it can be the case that the available resources out scale the dataset, and as a consequence we have some nodes remaining idle when executing only one step. 

\subsection{Speculative Execution}
Modern CPUs use speculative execution to improve performance by predicting which instructions are likely to be needed in the near future and executing them before they are actually required. This is especially useful for conditional branch instructions, as their outcome determines the instructions that will follow. By speculating the result, the future instructions can be pre-loaded, thus avoiding pipeline disruption and CPU idling time, waiting for the correct instruction to be retrieved from main memory. In fact, without branch prediction, a processor must stall whenever there are unresolved branch instructions, which can impose a substantial penalty on the performance of the processor since on average 20 percent of instructions are branches. Speculative execution gets significantly important, especially when there is a large gap between CPU and main memory speeds. If the accuracy of the predictions is sufficiently high, the offset cost of the computation of the prediction, is covered by the time gained by avoiding idling time.

Similarly, we aim to use the same approach in K-Means. As we saw, there is a cyclic dependency in its steps, the result of one step determines the result of the next ones. While computing the exact result of the current step, we would like to speculate its result, and already start computing the next ones, diving into the speculation path as long as possible.
