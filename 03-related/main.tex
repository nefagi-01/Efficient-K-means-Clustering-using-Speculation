\label{section:related_work}

K-Means clustering is a widely used unsupervised machine learning algorithm for data partitioning and clustering. This section provides an overview of the basic K-Means algorithms, the existing initialization techniques, the approximations which can be done to improve performances and the parallelization methods. We discuss the advantages and disadvantages of various approaches and explain how they can be applied in various scenarios. Furthermore, we present a comparison between the currently used approaches and the one we propose based on Speculation. Finally, we also show how our proposed approach can outperform the existing ones in terms of both accuracy and speed.
\subsection{K-Means++ initialization}
\label{section:K-Means_pp}

K-Means is sensible to initialization problems. It starts by randomly assigning a set of centroids, which can lead to suboptimal solutions if they are not well-chosen. To mitigate this, it is important to use an appropriate initialization strategy, such as k-means++, which elects initial cluster centroids using sampling based on an empirical probability distribution that reflects the points' contribution to the overall inertia. This offers a number of advantages over random initialization. It helps to avoid local minima, meaning that it can find the global optimum and produce better quality clusters compared to random initialization. Additionally, it may help to reduce the number of iterations needed to find the optimal solution, making it faster and more efficient than random initialization.
In Algorithm \ref{alg:K-Means_pp_initialization} we describe the steps for the centroid initialization.
\begin{algorithm}[H]
  \caption{K-Means++ Initialization}
  \label{alg:K-Means_pp_initialization}
  \begin{algorithmic}[1]
    \State Choose one center uniformly at random among the data points.
    \For{each data point x not chosen yet}
        \label{step:compute_Dx}
        \State Compute $D(x)$, the distance between x and the nearest center that has already been chosen.
    \EndFor
    \State Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to $D(x)^2$.
    \label{step:choose_centroid}
    \Repeat
        \State Steps \ref{step:compute_Dx} to \ref{step:choose_centroid}
    \Until k centers have not been chosen
    \State Return the initial centers have been chosen
  \end{algorithmic}
\end{algorithm}

We can see how the initialization Algorithm \ref{alg:K-Means_pp_initialization} involves the computation of the distances between each datapoint and the nearest center. This is an expensive operation, which is repeated every time a new centroid is sampled. Therefore, all the aforementioned benefits come to an initial time-cost we need to pay before running the actual algorithm. This can be prohibitive in particular applications, especially in case of large datasets.

\subsection{Approximations with Sampling. Mini-batch K-Means and online K-Means.}
\label{section:approximation}
Approximations with sampling are methods for increasing performances by using a subset of the data. In addition, these algorithms are useful when dealing with large datasets that would be too costly to process in one go. 
Mini-batch K-Means and online K-Means are two such methods. 
\begin{itemize}
    \item Mini-batch K-Means is an iterative algorithm that uses a subset of the data to update the cluster centers. At each K-Means iteration a new mini-batch is sampled.
    \item Online K-Means is a streaming algorithm that updates the cluster centers using one datapoint at the time.
\end{itemize}
Both of these methods are used to reduce the execution time and the amount of memory needed to run K-means, especially in case of big datasets. However, as we saw in section \ref{section:sampling}, this comes at a cost on the accuracy of the final centroids that are found. The fact we are not using the entire dataset in order to find the centroids leads to solutions with a higher inertia than what we could have obtained by using the entire dataset.

\subsection{Parallelization}
\label{section:parallelization}
An alternative solution for addressing K-Means on large datasets is to use parallelization with a distributed framework such as Spark. This approach takes advantage of distributed computing to process large datasets quickly and efficiently. However, there are two limitations to this approach, as discussed in [paper speculation]. Firstly, Amdahl's law suggests that the benefits of additional resources may be limited, as the increase in resources often results in diminishing returns in performance. Secondly, the data parallelization cannot break the dependency between the two steps in K-means mentioned in section \ref{section:dependencies}, meaning that a single step must be executed at a time, possibly leading to resource underutilization. In fact, in some scenarios it can be the case that the available resources out scale the dataset, and as a consequence we have some nodes remaining idle when executing only one step. 

\subsection{Speculative Execution}
The Speculation technique we use is based on the idea of Speculative Execution from the paper X, which defines a new query processing paradigm that accelerates inter-dependent queries using speculation and approximate query processing. The Speculative technique used allows to relax dependencies between queries, increase task parallelism, and reduce end-to-end latency. It works by detecting dependencies between outer and inner sub-queries and creating two separate execution paths: one for speculative execution and one for validation/repairs. The speculative execution path executes the outer query by resolving any cross-query conditions with the help of a branch predictor. At the same time, the validation/repair execution path performs three steps: (i) it fully computes the inner query, (ii) it validates the speculative decisions, and (iii) it repairs the results in case of mispredictions. This technique decouples the inner and outer queries and allows them to run in parallel or share data and operators if they process rows from the same table.

The main difference between the Speculative Execution technique used in paper X and the Speculative K-Means technique is that in the former, the speculation is used to break the dependency between the inner and outer query in order to increase parallelism, while in the latter, the speculation is used to predict possible future results (i.e: the centroids). Therefore, we are breaking the dependencies in a different sense, which is that we are not traversing the entire chain of dependencies anymore in order to arrive at the final result. Additionally, the correction phase in paper X ensures that the results of the query using the speculation technique are the same as the results of a vanilla execution, while in our approach the correction phase only ensures that the centroids obtained using the fast execution are better in terms of inertia than those of the slow execution. The centroids we obtain using the fast execution are an approximation of future centroids with similar inertia we may encounter in the slow execution many stages later, but they are very likely to be different.

Finally, we can see how the approach used in Speculative K-Means, reminds the approach used in modern CPUs where speculative execution is used to improve performance by predicting which instructions are likely to be needed in the near future and executing them before they are actually required. In the same way, while computing the correct solution of the current K-Means stage we start start approximately computing the next ones, diving into the speculation path as long as possible: if the results we find in the speculative path are better we take them and continue from there.


